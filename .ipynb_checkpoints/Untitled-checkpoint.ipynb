{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGDI\n",
    "\n",
    "This jupyter-notebook is used to show how row-based LGDI (can work better if missing value gap size is small) and col-based LGDI (can work better if missing value gap is large) work. These two methods are all based on Reshape Method 3. More details of the algorithms are introduced in our IEEE Sensor journal paper: \"Data Imputation for Multivariate Time Series Sensor Data with Large Gaps of Missing Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import random\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import os, sys\n",
    "currentdir = os.path.dirname(os.path.realpath('__file__'))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "import util\n",
    "\n",
    "# This is used to decide the K of K-fold cross validation\n",
    "CV = 5\n",
    "# decide the missing data for a matrix\n",
    "HOLE_PERCENT = 0.3\n",
    "\n",
    "# for boxcox transformation\n",
    "# Sometimes data transformation can help improve the accuracy\n",
    "LMBDA = 2\n",
    "DATA_TRANSFORM = True\n",
    "\n",
    "# -1 means use first available factor to reshape\n",
    "# or you can mannualy decide the reshape factor\n",
    "RESHAPE_FACTOR = -1\n",
    "# RESHAPE_FACTOR = 10\n",
    "\n",
    "# if ROW_BASED_RESHAPE = False, then this is col-based LGDI\n",
    "# if ROW_BASED_RESHAPE = True, then this is row-based LGDI\n",
    "ROW_BASED_RESHAPE = False\n",
    "\n",
    "# defines which chunk of the CV to visualize\n",
    "VISUALIZE_CHUNK = 1\n",
    "\n",
    "# LGDI is for multivariate data imputation.\n",
    "# this program checks accuracy for the variable with continous\n",
    "# missing gap. COL_NAME = 'q_cms', so we check imputation accuracy\n",
    "# for q_cms because we will create continous missing gap for\n",
    "# q_cms only. Other variable will also have missing values\n",
    "COL_NAME = 'q_cms'\n",
    "\n",
    "# this is the Extreme Event Flag Column in Figure 4.\n",
    "# this can be created with different extreme value detection algorithms\n",
    "COL_THRESHOLD = 'Basic_Threshold'\n",
    "\n",
    "# this is an input parameter for MICE Imputation function\n",
    "MAX_ITER = 10\n",
    "\n",
    "# this is the continous data gap percentage\n",
    "TESTING_PERCENTAGE = 1/float(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we demo how LGDI can be applied to MICE\n",
    "# MICE can have different kernals\n",
    "\n",
    "# KERNEL = DecisionTreeRegressor(max_features='sqrt')\n",
    "# KERNEL = KNeighborsRegressor(n_neighbors=3)\n",
    "# KERNEL = SVR(C=0.5, epsilon=0.25, gamma='scale')\n",
    "# KERNEL = MLPRegressor(learning_rate='adaptive', max_iter=500)\n",
    "KERNEL = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have three sample datasets\n",
    "df_with_time = pd.read_csv(\"Data/wade.csv\",sep=',')\n",
    "# df_with_time = pd.read_csv(\"Data/allStreamData_Hungerford_no_nan_basic_threshold_top_ten_feature.csv\",sep=',')\n",
    "# TODO\n",
    "# df_with_time = pd.read_csv(\"Data/allStreamData_Potash_top_ten_no_nan_basic_threshold.csv\",sep=',')\n",
    "\n",
    "# we may have some missing values before we create holes and gaps\n",
    "# the estimations of these missing values cannot be compared with ground truth\n",
    "# so we just removed these records\n",
    "# .reset_index() make sure index starting from 0\n",
    "# this will insert the previous index as first col\n",
    "df_no_nan = df_with_time.dropna().reset_index()\n",
    "\n",
    "\n",
    "# In the sample data, the first two cols are index and timestamp\n",
    "# the last col is threshold, we need to remove these three cols\n",
    "df_no_nan_no_time = df_no_nan.iloc[:,2:-1]\n",
    "df_no_nan_no_time_copy = df_no_nan_no_time.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of all outliers\n",
    "results = {'alarms':[], 'normal':[]}\n",
    "results['alarms'] = df_no_nan.index[df_no_nan[COL_THRESHOLD]==1].tolist()\n",
    "# create normal events index list\n",
    "total_data_size = df_no_nan.shape[0]\n",
    "results['normal'] = np.setdiff1d(range(total_data_size),results['alarms'])\n",
    "\n",
    "# each cross validation data size \n",
    "cv_data_size = int(total_data_size/CV)\n",
    "\n",
    "# this is original dataset with timestamps\n",
    "original = df_with_time.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently dealing with loop: 0\n",
      "current df shape is:(10000, 11)\n",
      "Qualified factors include: [5000, 2500, 2000, 1250, 1000, 625, 500, 400, 250, 200, 125, 100, 80, 50, 40, 25, 20, 16, 10, 8, 5, 4, 2]\n",
      "After reshape, the current shape is:(5000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9532115500217238\n",
      "currently dealing with loop: 1\n",
      "current df shape is:(10000, 11)\n",
      "Qualified factors include: [5000, 2500, 2000, 1250, 1000, 625, 500, 400, 250, 200, 125, 100, 80, 50, 40, 25, 20, 16, 10, 8, 5, 4, 2]\n",
      "After reshape, the current shape is:(5000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4119890198088092\n",
      "currently dealing with loop: 2\n",
      "current df shape is:(10000, 11)\n",
      "Qualified factors include: [5000, 2500, 2000, 1250, 1000, 625, 500, 400, 250, 200, 125, 100, 80, 50, 40, 25, 20, 16, 10, 8, 5, 4, 2]\n",
      "After reshape, the current shape is:(5000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.142103950847754\n",
      "currently dealing with loop: 3\n",
      "current df shape is:(10000, 11)\n",
      "Qualified factors include: [5000, 2500, 2000, 1250, 1000, 625, 500, 400, 250, 200, 125, 100, 80, 50, 40, 25, 20, 16, 10, 8, 5, 4, 2]\n",
      "After reshape, the current shape is:(5000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7689041473723952\n",
      "currently dealing with loop: 4\n",
      "current df shape is:(10000, 11)\n",
      "Qualified factors include: [5000, 2500, 2000, 1250, 1000, 625, 500, 400, 250, 200, 125, 100, 80, 50, 40, 25, 20, 16, 10, 8, 5, 4, 2]\n",
      "After reshape, the current shape is:(5000, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiwu1990/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py:699: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-770850e7070c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0marray_filled_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0marray_filled_extreme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp_extreme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_extreme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0marray_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_normal_extreme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_extreme_row_index_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_normal_row_index_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_filled_extreme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_filled_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreshape_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_corr_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 )\n\u001b[0;32m--> 664\u001b[0;31m                 Xt, estimator = self._impute_one_feature(\n\u001b[0m\u001b[1;32m    665\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                     \u001b[0mmask_missing_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_feat_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmissing_row_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmissing_row_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# if no missing values, don't predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    664\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r_2_list = []\n",
    "vis_prediction = []\n",
    "vis_gap = []\n",
    "x_index = []\n",
    "for i in range(CV):\n",
    "    print(\"currently dealing with loop:\", i)\n",
    "    \n",
    "    df_no_nan_no_time_backup = df_no_nan_no_time_copy.copy()\n",
    "    if DATA_TRANSFORM:\n",
    "        # data tranformation\n",
    "        df_no_nan_no_time_backup[COL_NAME] = boxcox(df_no_nan_no_time_backup[COL_NAME],LMBDA)\n",
    "    \n",
    "    # create holes for all variables\n",
    "    util.make_holes_matrix(df_no_nan_no_time_backup,HOLE_PERCENT,COL_NAME)\n",
    "\n",
    "    # start and end of continous missing value gap\n",
    "    start_index = i*cv_data_size\n",
    "    end_index = start_index + cv_data_size\n",
    "\n",
    "\n",
    "    # create holes for normal and extreme events\n",
    "    for index in range(start_index, end_index):\n",
    "        df_no_nan_no_time_backup[COL_NAME].iloc[index] = np.nan\n",
    "\n",
    "\n",
    "    # if one row has an extreme event, then treat this line as extreme event\n",
    "    # if ROW_BASED_RESHAPE == False, this is Col-based LGDI\n",
    "    if ROW_BASED_RESHAPE == False:\n",
    "        print('current df shape is:'+str(df_no_nan_no_time_backup.shape))\n",
    "        # reshape\n",
    "        reshape_df, factor = util.ccrm(df_no_nan_no_time_backup, RESHAPE_FACTOR)\n",
    "        reshape_normal_row_index_list, reshape_extreme_row_index_list = util.convert_row_index_col_based_reshape(results['alarms'], factor, df_no_nan_no_time_backup.shape[0])\n",
    "        if len(reshape_normal_row_index_list) != 0:\n",
    "            df_normal = reshape_df.iloc[reshape_normal_row_index_list]\n",
    "            df_extreme = reshape_df.iloc[reshape_extreme_row_index_list]\n",
    "\n",
    "            imp_normal = IterativeImputer(max_iter=MAX_ITER, estimator=KERNEL)\n",
    "            imp_extreme = IterativeImputer(max_iter=MAX_ITER, estimator=KERNEL)\n",
    "\n",
    "            array_filled_normal = imp_normal.fit_transform(df_normal)\n",
    "            array_filled_extreme = imp_extreme.fit_transform(df_extreme)\n",
    "\n",
    "            array_filled = util.combine_normal_extreme(reshape_df.shape[0], reshape_extreme_row_index_list, reshape_normal_row_index_list, array_filled_extreme, array_filled_normal, reshape_df.shape[0],reshape_df.shape[1])\n",
    "        else:\n",
    "            # when no normal events\n",
    "            df_extreme = reshape_df.iloc[reshape_extreme_row_index_list]\n",
    "            imp_extreme = IterativeImputer(max_iter=MAX_ITER, estimator=KERNEL)\n",
    "            array_filled = imp_extreme.fit_transform(df_extreme)\n",
    "\n",
    "        # reverse reshape\n",
    "        df_tmp = pd.DataFrame(array_filled)\n",
    "        original_n1 = df_no_nan_no_time_backup.shape[0]\n",
    "        original_n2 = df_no_nan_no_time_backup.shape[1]\n",
    "        original_df = util.ccrm_reverse(df_tmp, original_n1, original_n2)\n",
    "        array_filled = original_df.to_numpy()\n",
    "    # if ROW_BASED_RESHAPE == True, this is Row-based LGDI\n",
    "    else:\n",
    "        print('current df shape is:'+str(df_no_nan_no_time_backup.shape))\n",
    "        # by default, the minimum factor will be used\n",
    "        # it means the shape will be balanced (row and col number not too small)\n",
    "        reshape_df, factor = util.row_based_reshape(df_no_nan_no_time_backup, RESHAPE_FACTOR)\n",
    "\n",
    "        reshape_normal_row_index_list, reshape_extreme_row_index_list = util.convert_row_index_row_based_reshape(results['alarms'], factor, df_no_nan_no_time_backup.shape[0])\n",
    "\n",
    "        df_normal = reshape_df.iloc[reshape_normal_row_index_list]\n",
    "        df_extreme = reshape_df.iloc[reshape_extreme_row_index_list]\n",
    "\n",
    "        imp_normal = IterativeImputer(max_iter=MAX_ITER, estimator=KERNEL)\n",
    "        imp_extreme = IterativeImputer(max_iter=MAX_ITER, estimator=KERNEL)\n",
    "\n",
    "        array_filled_normal = imp_normal.fit_transform(df_normal)\n",
    "        array_filled_extreme = imp_extreme.fit_transform(df_extreme)\n",
    "\n",
    "        array_filled = util.combine_normal_extreme(reshape_df.shape[0], reshape_extreme_row_index_list, reshape_normal_row_index_list, array_filled_extreme, array_filled_normal, reshape_df.shape[0],reshape_df.shape[1])\n",
    "        # # transpose to avoid multiple holes for the same row\n",
    "        # array_filled = imp.fit_transform(reshape_df.T)\n",
    "        # array_filled = imp.fit_transform(reshape_df)\n",
    "        # reverse reshape\n",
    "        df_tmp = pd.DataFrame(array_filled)\n",
    "        original_n1 = df_no_nan_no_time_backup.shape[0]\n",
    "        original_n2 = df_no_nan_no_time_backup.shape[1]\n",
    "\n",
    "        # original_df = util.row_based_reshape_reverse(df_tmp.T, original_n1, original_n2)\n",
    "        original_df = util.row_based_reshape_reverse(df_tmp, original_n1, original_n2)\n",
    "        array_filled = original_df.to_numpy()\n",
    "\n",
    "\n",
    "    # calculate accuracy between backup and df\n",
    "    ground_truth = []\n",
    "    prediction = []\n",
    "\n",
    "    if DATA_TRANSFORM:\n",
    "        # convert array to float\n",
    "        array_filled = array_filled.astype(float)\n",
    "        # back transformation\n",
    "        # we may generate nan value\n",
    "        array_filled[:,0] = np.nan_to_num(inv_boxcox(array_filled[:,0],LMBDA))\n",
    "        # array_filled[:,0] = array_filled[:,0]*10\n",
    "\n",
    "    for index in range(start_index, end_index):\n",
    "        # rule 1: prediction should be above 0\n",
    "        if array_filled[:,0][index]<0:\n",
    "            prediction.append(0)\n",
    "            array_filled[:,0][index] = 0\n",
    "        else:\n",
    "            prediction.append(array_filled[:,0][index])\n",
    "\n",
    "        ground_truth.append(original[COL_NAME].iloc[index])\n",
    "\n",
    "\n",
    "    tmp_r_2 = r2_score(ground_truth.copy(), prediction.copy())\n",
    "    print(tmp_r_2)\n",
    "    r_2_list.append(tmp_r_2)\n",
    "\n",
    "\n",
    "    if i+1 == VISUALIZE_CHUNK:\n",
    "        x_index = range(start_index, end_index)\n",
    "        vis_prediction = array_filled[:,0].copy()\n",
    "        vis_gap = df_no_nan_no_time_backup[COL_NAME].copy()\n",
    "\n",
    "        \n",
    "# calculate R2 value\n",
    "avg_r2 = sum(r_2_list) / len(r_2_list)\n",
    "print(\"average R^2 is: \", avg_r2)\n",
    "print(r_2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell show the visualization results\n",
    "\n",
    "x_time = util.get_time_stamp_array(original)\n",
    "# original\n",
    "y1 = original[COL_NAME]\n",
    "# with gap\n",
    "y1_gap = vis_gap\n",
    "\n",
    "# filled with predictions\n",
    "y1_filled = vis_prediction \n",
    "\n",
    "# +1 because the last one will be missing if not\n",
    "original_gap_data = df_no_nan[COL_NAME].tolist()[x_index[0]:x_index[-1]+1]\n",
    "\n",
    "# # vis\n",
    "plt.scatter( x_time[x_index], original_gap_data, marker=\"x\", label=COL_NAME+\"_original\")\n",
    "plt.scatter( x_time, y1_gap, marker=\"v\", label=COL_NAME+\"_with_gap\")\n",
    "plt.scatter( x_time[x_index], y1_filled[x_index], marker=\"+\", label=COL_NAME+\"_filled\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
